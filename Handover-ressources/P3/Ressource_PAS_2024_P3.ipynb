{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*****PAS 2024 Ressources P3 : leak detection (oil & gas)*****"
      ],
      "metadata": {
        "id": "LK78lLsuLGWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this file, we provide some libraries and functions that you will need for the project. However, don't limit yourself to what is given here; it's up to you to explore and find additionnal resources to successfully complete the project."
      ],
      "metadata": {
        "id": "q10fmTuFLWsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Libraries**"
      ],
      "metadata": {
        "id": "n2XTiTAYTfBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # importing and exporting data\n",
        "import numpy as np  # numerical computing\n",
        "import matplotlib.pyplot as plt  # Plot for data visualisation\n",
        "import seaborn as sns   # statistical data visualisation\n",
        "from sklearn.model_selection import train_test_split # data separation: train and test data\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder  # data normalisation\n",
        "from imblearn.over_sampling import SMOTE # Oversampling\n",
        "from sklearn.compose import make_column_selector # Separate numerical and categorical variables\n",
        "\n",
        "# Classical machine learning models for classification\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb # gradient boosting model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # model evaluation\n",
        "\n",
        "# Deep learning models for classification with tensorflow. You can also use Pytorch\n",
        "from tensorflow.keras.models import Sequential # neural networl model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization # layers neural network\n",
        "from tensorflow.keras.callbacks import EarlyStopping # callback that monitors a specified metric during training and stops the training process when that metric stops improving.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "Wi6coC15LFWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data import**"
      ],
      "metadata": {
        "id": "N1dnE7JLZXj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Path  = 'File path / file_name.csv'\n",
        "data = pd.read_csv(path)\n",
        "data"
      ],
      "metadata": {
        "id": "wcvkyWVrZWn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Exploration**"
      ],
      "metadata": {
        "id": "f5hT9wrzbmr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data information: provide variables, number of non-null values and and data type"
      ],
      "metadata": {
        "id": "_c03b08laMva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "55ykNLLbTdD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data size"
      ],
      "metadata": {
        "id": "As58k5tubzvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "-AKUA-GlTdGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a pie chart showing the distribution of different data types, if necessary."
      ],
      "metadata": {
        "id": "Inukrsr4cDlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes.value_counts().plot.pie()"
      ],
      "metadata": {
        "id": "TaqI6M8KTdK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize missing values"
      ],
      "metadata": {
        "id": "pWkIcRm0c2Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(data.isna(), cbar=False) # shows where missing values (NaNs) occur in the DataFrame"
      ],
      "metadata": {
        "id": "OpMgDZdLTdNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the distribution of values in the target"
      ],
      "metadata": {
        "id": "08lU6d8idX69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"target\"].value_counts(normalize=True)  # proportion of each unique values in the target column"
      ],
      "metadata": {
        "id": "EF3vrGHITdRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proportion of missing values (NaNs) in each variable"
      ],
      "metadata": {
        "id": "wdDfiGnJexAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"variable\"].isna().sum()/data.shape[0]"
      ],
      "metadata": {
        "id": "WyJerKTNTdWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If necessary and for visualization of each categorical, create a pie chart for each categorical (object) column in the DataFrame data, showing the distribution of unique values within that column."
      ],
      "metadata": {
        "id": "RDtnIosOfGl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in data.select_dtypes('object'):\n",
        "plt.figure()\n",
        "data[col].value_counts().plot.pie() # Each chart displays the proportions of different categories in a visually intuitive way."
      ],
      "metadata": {
        "id": "5pKvPMRjfCUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between target and variables**"
      ],
      "metadata": {
        "id": "x5ufBbYAh-k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Create subset with leak and no leak"
      ],
      "metadata": {
        "id": "GGv-ugSmf-nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(['id'], axis=1) # you need to drop the id variable\n",
        "\n",
        "leak_yes = data[data['target']==1]\n",
        "leak_no = data[data['target']==0]\n",
        "leak_yes.describe()\n",
        "leak_no.describe()"
      ],
      "metadata": {
        "id": "9zdnki4DfCWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relationship between target and categorical variables"
      ],
      "metadata": {
        "id": "6Q_icbx4h1__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "D2OHJfCJhqXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates distribution plots for each float column in the DataFrame data, comparing the distributions of the two groups.\n",
        "for col in data.select_dtypes('float'):\n",
        "plt.figure()\n",
        "sns.distplot(leak_yes[col], label='leak')\n",
        "sns.distplot(leak_no[col], label='no leak')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "CtgBOWUffCb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relationship between variables**"
      ],
      "metadata": {
        "id": "iInNrhxhjjto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlations"
      ],
      "metadata": {
        "id": "6S-JZrm4jpVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between varaible\n",
        "sns.clustermap(data.corr())"
      ],
      "metadata": {
        "id": "af762bTxjTgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization tool to explore relationships between multiple variables, allowing you to quickly identify patterns, correlations, and potential outliers.\n",
        "sns.pairplot(data)"
      ],
      "metadata": {
        "id": "W0ArFBqQjTjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can also look at the relationship between each variable and the target**"
      ],
      "metadata": {
        "id": "L_OvCKDol4yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If necessary, replace missing values by median. You can also use the mean or others statistics"
      ],
      "metadata": {
        "id": "PwmkF-Kwk0vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "median = data[\"variable with missing values\"].median()\n",
        "data[\"variable with missing values\"].fillna(median, inplace = True)\n",
        "data"
      ],
      "metadata": {
        "id": "qXSB2OaCjTnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If necessary, separate numerical and categorical variables"
      ],
      "metadata": {
        "id": "QpE1hbtQln3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns_selector = make_column_selector(dtype_exclude=object)\n",
        "categorical_columns_selector = make_column_selector(dtype_include=object)\n",
        "\n",
        "numerical_columns = numerical_columns_selector(data)\n",
        "categorical_columns = categorical_columns_selector(data)\n",
        "numerical_columns\n",
        "categorical_columns"
      ],
      "metadata": {
        "id": "F_7KS9wBjTph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing"
      ],
      "metadata": {
        "id": "3EnYeSwGmJOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "numerical_preprocessor = StandardScaler()\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('onehotencoder',categorical_preprocessor,categorical_columns),\n",
        "    ('standardscaler',numerical_preprocessor,numerical_columns)])\n",
        "\n",
        "data = preprocessor.fit_transform(data)\n",
        "data"
      ],
      "metadata": {
        "id": "Vg17r2kgl9zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data separation for train and validation"
      ],
      "metadata": {
        "id": "riPX2H5pmiug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train, data_val, target_train, target_val = train_test_split(data, target, test_size =0.30, random_state = 42)\n",
        "target_train.value_counts() # the size of training set"
      ],
      "metadata": {
        "id": "HIB7pOvkl92p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If necessary, oversampling the training set. Useful for unbalanced classes"
      ],
      "metadata": {
        "id": "Zr312TIinESi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add synthetic data to have balanced classes for training.\n",
        "\n",
        "SMOTE = SMOTE()\n",
        "data_train_smote, target_train_smote = SMOTE.fit_resample(data_train, target_train)\n",
        "target_train_smote.value_counts()"
      ],
      "metadata": {
        "id": "0qMvYWDGl97P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training model**"
      ],
      "metadata": {
        "id": "ptQ3OJ7Fojft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_used() # model_used can be KNN, SVM, Random Forest,........\n",
        "model.fit(data_train, target_train)\n",
        "mdodel.fit(data_train_smote, target_train_smote) # if oversampling"
      ],
      "metadata": {
        "id": "WGNeyAG1l9-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model predict**"
      ],
      "metadata": {
        "id": "MyKh6H9Qo3m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_pred  = model.predict(data_val)"
      ],
      "metadata": {
        "id": "J8-CuRHDjTvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "6sDkF5NgpTyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MkiBAfpTptBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report # provides key metrics such as precision, recall, F1 score, and support for each class in your classification problem.\n",
        "from sklearn import metrics\n",
        "fpr, tpr, thresholds = metrics.roc_curve(target_val, target_pred)\n",
        "\n",
        "print('AUC : ', metrics.auc(fpr, tpr))\n",
        "print(classification_report(y_test, y_pred))\n",
        "plt.plot(fpr, tpr, label ='model_name')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ABWrMA83pXP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix is also useful."
      ],
      "metadata": {
        "id": "6b-gPntYqpZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We also recommend you to use the powerful library GridSearchCV in order to optimize the parameters of the used model.**"
      ],
      "metadata": {
        "id": "ptGHo_qMqMyd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqTDNFs3pXX2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}